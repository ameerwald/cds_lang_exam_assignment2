{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "Run the ``` bash setup.sh``` first for this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# system tools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# data munging tools\n",
    "import pandas as pd\n",
    "import utils.classifier_utils as clf\n",
    "\n",
    "# Machine learning stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# saving models \n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X,y):\n",
    "    # making the train/test split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,               # texts for the model\n",
    "                                                        y,               # classification labels\n",
    "                                                        test_size=0.2,   # create an 80/20 split (20% test data, 80% train data)\n",
    "                                                        random_state=42) # random state for reproducibility - like set.seed() in R\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(X_train, X_test):\n",
    "    # creating vectorizer object - which makes the text features into numerical vectors and makes it easier/faster to work with  \n",
    "    vectorizer = TfidfVectorizer(ngram_range = (1,2),    # unigrams and bigrams, but can be more \n",
    "                                lowercase =  True,       # making everything lowercase \n",
    "                                max_df = 0.95,           # removes words in more than 95% of documents, the super common ones \n",
    "                                min_df = 0.05,           # removes words in less than 5% of documents, the super rare ones and possibly misspelled words \n",
    "                                max_features = 500)      # keep only top 500 features\n",
    "    # fit the vectorizer to the training data - calculates the mean and variance of each feature \n",
    "    # then transforms all the features with the respective mean and variance to scale the training data \n",
    "    X_train_feats = vectorizer.fit_transform(X_train)\n",
    "    # transform test data - using the same mean and variance calculated from the training data to transform the test data \n",
    "    # do not want to also fit the test data or the model will learn from that too and will not be an accurate indicator of how the model performs \n",
    "    X_test_feats = vectorizer.transform(X_test)\n",
    "    # save the model \n",
    "    dump(vectorizer, os.path.join(\"..\", \"models\", \"tfidf_vectorizer.joblib\"))\n",
    "    return vectorizer, X_train_feats, X_test_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " def logistic_classifier():\n",
    "    # trained on the training data - logistic regression\n",
    "    classifier = LogisticRegression(random_state=42)\n",
    "    classifier.fit(X_train_feats, y_train)\n",
    "    # using test data to see predictions, based on training data \n",
    "    y_pred = classifier.predict(X_test_feats)\n",
    "    # get predictions \n",
    "    y_pred = classifier.predict(X_test_feats)\n",
    "    # show metrics for how model is performing \n",
    "    classifier_metrics = metrics.classification_report(y_test, y_pred)\n",
    "    print(classifier_metrics)\n",
    "    # saving the model\n",
    "    dump(classifier, \"LR_classifier.joblib\")\n",
    "    return classifier, classifier_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_classifier(): \n",
    "    # trained on the training data - neural network\n",
    "    classifier = MLPClassifier(activation = \"logistic\", # function for the hidden layer, logistic = sigmoid function \n",
    "                            hidden_layer_sizes = (20,), # number chosen, in this case 20 means 20 neurons in the 20 hidden layers \n",
    "                            max_iter=1000, # max number of iterations until model converges, may be before it reaches this number \n",
    "                            random_state = 42) \n",
    "    # classifier is fit to the training data to learn \n",
    "    classifier.fit(X_train_feats, y_train)\n",
    "    # get predictions - classifier is now used to get predictions from the test data  \n",
    "    y_pred = classifier.predict(X_test_feats)\n",
    "    # save the model \n",
    "    dump(classifier, os.path.join(\"..\", \"models\", \"NNclassifier.joblib\"))\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics():\n",
    "    # save the report to a file\n",
    "    with open(os.path.join(\"..\", \"out\", \"classification_report.txt\"), \"w\") as f:\n",
    "        f.write(classifier_metrics)\n",
    "    return none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "logistic_classifier() takes 0 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m vectorizer, X_train_feats, X_test_feats \u001b[39m=\u001b[39m vectorize(X_train, X_test)\n\u001b[1;32m     13\u001b[0m \u001b[39m# logistic regression classifier \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m classifier, classifier_metrics \u001b[39m=\u001b[39m logistic_classifier(X_train_feats, y_train, X_train_feats, y_test)\n\u001b[1;32m     15\u001b[0m save_data \u001b[39m=\u001b[39m save_metrics()\n",
      "\u001b[0;31mTypeError\u001b[0m: logistic_classifier() takes 0 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(\"..\",\"in\",\"fake_or_real_news.csv\")\n",
    "data = pd.read_csv(filename, index_col=0)\n",
    "# these column names will be specific to whatever dataset is loaded in\n",
    "X = data[\"text\"]\n",
    "y  = data[\"label\"] \n",
    "\n",
    "\n",
    "for file in data:\n",
    "    # splitting into train/test data \n",
    "    X_train, X_test, y_train, y_test = train_test(X,y)\n",
    "    # vectorizing \n",
    "    vectorizer, X_train_feats, X_test_feats = vectorize(X_train, X_test)\n",
    "    # logistic regression classifier \n",
    "    classifier, classifier_metrics = logistic_classifier(X_train_feats, y_train, X_train_feats, y_test)\n",
    "    save_data = save_metrics()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\" Function to load in data \n",
    "\n",
    "    Returns:\n",
    "        X: the text data \n",
    "        y: the labels of that data \n",
    "    \"\"\"    \n",
    "    # making varibles global so they can be used in other functions \n",
    "    global X \n",
    "    global y\n",
    "    filename = os.path.join(\"..\",\"in\",\"fake_or_real_news.csv\")\n",
    "    data = pd.read_csv(filename, index_col=0)\n",
    "    # these column names will be specific to whatever dataset is loaded in \n",
    "    X = data[\"text\"]\n",
    "    y  = data[\"label\"]\n",
    "    return X,y # is this right? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
